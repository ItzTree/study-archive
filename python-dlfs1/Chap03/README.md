# Chap03: 신경망

퍼셉트론으로 복잡한 함수도 표현할 수 있지만, 가중치를 설정하는 작업은 사람이 수동으로 해야 한다. 하지만, **신경망**을 이용하여 가중치가 적절한 값을 갖게 할 수 있다.  
```math
y = \begin{cases}
0 & (b\cdot 1 + w_1x_1 + w_2x_2 \leq 0) \\
1 & (b\cdot 1 + w_1x_1 + w_2x_2 > 0)
\end{cases}
```
편향 b를 가중치가 b이고 입력이 1인 뉴런으로 보면 위와 같이 표현할 수 있다. 함수 $h(x)$를 다음과 같이 정의하자.  
```math
h(x) = \begin{cases}
0 & (x \leq 0) \\
1 & (x > 0)
\end{cases}
```
그렇다면 퍼셉트론은 아래의 식으로 표현할 수 있다.  
$$y = h(b + w_1x_1 + w_2x_2)$$
이렇게 $h(x)$와 같이 입력 신호의 총합을 출력 신호로 바꾸는 함수를 **활성화 함수**라고 한다.

### 활성화 함수
퍼셉트론에서 0이라는 임계값을 경계로 출력이 0 또는 1로 바뀌었는데, 이런 함수를 **계단 함수**라고 한다.  
신경망에서 자주 사용하는 함수로는 **시그모이드 함수**가 있다.
```math
h(x) = \frac{1}{1 + exp(-x)}
```
계단 함수는 0을 경계로 출력이 갑자기 바뀌지만, 시그모이드 함수의 연속성이 신경망 학습에서 매우 중요한 역할을 한다.  
최근에는 시그모이드 함수 대신 **ReLU 함수**를 주로 이용하기도 한다. $max(0, x)$로 간단하게 나타낼 수 있다.

### 3층 신경망
퍼셉트론은 다시 가중치 합 $a$와 활성화 함수 $h$에 의해 변환된 신호 $z$로 나타낼 수 있다.
$$a = w_1x_1 + w_2x_2 + b$$
$$z = h(a)$$
1층의 1번째 뉴런 $a^{(1)}_1$의 값을 표현하는 과정을 알아보자.  
```math
a^{(1)}_1 = w^{(1)}_{11}x_1 + w^{(1)}_{12}x_2 + b^{(1)}_1
```
행렬 곱을 이용하여 다음 식으로 간소화 할 수 있다.
```math
A^{(1)} = XW^{(1)} + B^{(1)}
```
가중치 합 결과 $A$를 활성화 함수를 통해 다음 은닉층의 입력 $Z$로 바꿔준다. 이 과정을 반복하다가 마지막 은닉층에서 출력층으로 갈 때 사용하는 활성화 함수는 어떤 문제냐에 따라 달라진다. **회귀**에는 항등함수를 사용하고, **분류**에는 소프트맥스 함수를 사용한다.  