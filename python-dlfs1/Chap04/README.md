# Chap04: 신경망 학습

**학습**이란 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것을 뜻한다. **손실 함수**의 결과값을 가장 작게 만드는 가중치를 찾아야 한다.  

### 데이터 분리
머신러닝 문제는 데이터를 훈련 데이터(training data)와 시험 데이터(test data)로 나눠 학습을 하는 것이 일반적이다. 훈련 데이터만 사용하여 최적의 매개변수를 찾고, 시험 데이터를 사용하여 훈련한 모델의 성능을 평가한다. 이 때, 하나의 데이터셋에만 지나치게 최적화된 **오버피팅(overfitting)**을 피해야 한다.

### 손실 함수
신경망이 최적의 매개변수를 찾기 위해 **손실 함수(loss function)**를 사용한다. 가장 많이 쓰이는 손실 함수로는 **평균 제곱 오차(Mean Squared Error)**가 있고, 식은 다음과 같다.
```math
E = \frac{1}{2}\sum_{k}(y_k-t_k)^2
```
신경망의 출력 `y`와 정답 레이블 `t`가 아래와 같은 값을 갖는다고 하자.  
```
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
```
이는 신경망이 답을 2라고 할 확률이 0.6임과 실제 정답은 2임을 나타낸다. 평균 제곱 오차는 추정 값 y와 정답 레이블 t의 차를 제곱하고 그 총합을 구한다. 이 값이 작을수록 정답에 가깝다고 판단한다.  

다른 손실 함수로 **교차 엔트로피 오차(Cross Entropy Error)**도 자주 이용한다.
```math
E = -\sum_{k} t_k\log y_k
```
이 CEE도 마찬가지로 값이 작을수록 정답에 가깝다고 판단한다.