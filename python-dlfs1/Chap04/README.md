# Chap04: 신경망 학습

**학습**이란 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것을 뜻한다. **손실 함수**의 결과값을 가장 작게 만드는 가중치를 찾아야 한다.  

### 데이터 분리
머신러닝 문제는 데이터를 훈련 데이터(training data)와 시험 데이터(test data)로 나눠 학습을 하는 것이 일반적이다. 훈련 데이터만 사용하여 최적의 매개변수를 찾고, 시험 데이터를 사용하여 훈련한 모델의 성능을 평가한다. 이 때, 하나의 데이터셋에만 지나치게 최적화된 <b>오버피팅(overfitting)</b>을 피해야 한다.

### 손실 함수
신경망이 최적의 매개변수를 찾기 위해 <b>손실 함수(loss function)</b>를 사용한다. 가장 많이 쓰이는 손실 함수로는 <b>평균 제곱 오차(Mean Squared Error)</b>가 있고, 식은 다음과 같다.
```math
E = \frac{1}{2}\sum_{k}(y_k-t_k)^2
```
신경망의 출력 `y`와 정답 레이블 `t`가 아래와 같은 값을 갖는다고 하자.  
```
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
```
이는 신경망이 답을 2라고 할 확률이 0.6임과 실제 정답은 2임을 나타낸다. 평균 제곱 오차는 추정 값 y와 정답 레이블 t의 차를 제곱하고 그 총합을 구한다. 이 값이 작을수록 정답에 가깝다고 판단한다.  

다른 손실 함수로 <b>교차 엔트로피 오차(Cross Entropy Error)</b>도 자주 이용한다.
```math
E = -\sum_{k} t_k\log y_k
```
이 CEE도 마찬가지로 값이 작을수록 정답에 가깝다고 판단한다.

### 미니배치 학습
모든 훈련 데이터를 대상으로 손실 함수 값을 모두 구해야 하므로, CEE를 다음과 같이 표현할 수 있다.
```math
E = -\frac{1}{N}\sum_{n}\sum_{k}t_{nk}\log y_{nk}
```
모든 데이터를 대상으로 일일이 구하기는 힘드므로, 일부를 추려 전체의 근사치로 활용할 수 있겠다. 이 훈련 데이터의 일부를 <b>미니배치(mini-batch)</b>라고 한다.  

신경망 학습에서는 최적의 가중치를 찾기 위해 손실 함수값이 가능하면 최소가 되도록 해야 한다. 이 때, **미분**을 사용하게 된다. 가중치를 아주 조금 변화시켰을 때, 손실 함수가 어떻게 바뀌느냐이다.  
미분 값이 음수면 가중치를 양의 방향으로 변화시키거나 미분 값이 양수면 가중치를 음의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있다.

하나의 변수에 대해서는 미분을 사용하지만, 여러 변수로 이루어진 경우에는 **편미분**을 사용한다. 모든 변수의 편미분을 벡터로 정리한 것을 <b>기울기(gradient)</b>라고 하는데, 이 기울기가 가리키는 쪽은 특정 구역(local)에서 출력 값을 가장 크게 줄이는 방향이다.  

### 경사하강법
기울기를 잘 활용해 함수의 최솟값을 찾는 것을 경사법이라고 한다. 하지만 기울기가 가리키는 곳이 함수의 최솟값인지는 보장할 수 없다. 이 경사법을 수식으로 나타내보자.
```math
x_i = x_i - \eta \frac{\partial f}{\partial x_i}
```
$\eta$는 한 번의 학습으로 매개변수 값을 얼마나 갱신할지를 정하는 <b>학습률(learning rate)</b>이다. 학습률이 너무 크면 큰 값으로 발산해버리고, 너무 작으면 거의 갱신되지 않은채 종료된다. 따라서, 학습률을 적절히 설정하는 것이 중요하다.  
수치 미분을 이용해 가중치 매개변수의 기울기를 구할 수 있다. 구현은 간단하지만, 시간이 오래걸린다는 단점이 있다. 하지만, 오차역전파법을 이용하면 기울기를 빠르게 구할 수 있다.  

### 학습 알고리즘 구현하기
1. 미니배치  
    훈련 데이터 중 일부를 무작위로 가져온다. 이 미니배치의 손실 함수값을 줄이는 것이 목표이다.
2. 기울기 산출  
    미니배치의 손실 함수값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수값을 가장 작게 하는 방향을 가리킨다.
3. 매개변수 갱신  
    가중치 매개변수를 기울기 방향으로 살짝 갱신한다.
4. 반복  
    1~3단계를 반복한다.

이 경사하강법은 데이터를 미니배치로 무작위 선정하기 때문에 <b>확률적 경사 하강법(stochastic gradient descent)</b>이라고 한다.  

### 평가하기
신경망 학습의 원래 목표는 범용적인 능력을 익히는 것이다. 범용 능력을 평가하라면 훈련 데이터에 포함되지 않은 데이터로 평가해봐야 한다. 한 에폭(epoch) 단위로 데이터에 대한 정확도를 기록한다.